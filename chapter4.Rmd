# Clustering and classification

```{r}
date()
```
Data for this chapter is the Boston dataset from the MASS R package.It contains information collected by the U.S Census Service concerning housing in the area of Boston. Let's load the data and look at the structure and dimensions.

```{r}
# access the MASS package
library(MASS)

# load the data
data("Boston")

# explore the dataset
dim(Boston)
str(Boston)
summary(Boston)

```
The Boston dataset has 506 observations of 14 variables. In this Assignment I focus on the variable "crim" that is the per capita crime rate by town. More information about the dataset can be found e.g. [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html)  

  **Graphical overview**  

Here we see a graphical summary of all the 14 variables paired with each other. The scatter plots are rather small as there is quit a few variables, but even in these tiny graphs  we can see that there are some correlations between variables. Let's print out and visualize a correlation matrix and look at those correlations a little closer.

```{r}

# plot matrix of the variables

pairs(Boston)

```
  
**Correlation matrix**  
In the correlation matrix we see that there are some pretty strong correlations between some of the variables. The strongest I think is the correlation between accessibility to radial highways (rad) and full-value property-tax rate (tax) (r=0,91) meaning that properties with better accessibility to radial highways have higher tax rate.  
Correlation matrix with this many variables is not the easiest way to quickly glimpse the correlations though, so let's visualize it to make it easier to spot the interesting correlations.
```{r}

# calculate the correlation matrix and round it
library(tidyr)
cor_matrix <- cor(Boston) %>% round(digits=2)


# print the correlation matrix
cor_matrix
```

In this visualization the bigger and darker the dot the stronger the correlation. Blue dots refers to positive correlation and red dots to negative. For example average number of rooms per dwelling (rm) has high positive correlation with median value of owner-occupied homes (medv) which is quite obvious, I guess. Interesting negative correlation is found for example between the proportion of older buildings (age = proportion of owner-occupied units built prior to 1940) and distance to employment centers (dis = weighted mean of distances to five Boston employment centres).  
As we are interested in crime rate in this assignment let's have a look at its correlations to other variables. The strongest correlation to crime rate is with accessibility to radial highways (r=0,63) and the second strongest is with full-value property-tax rate (r = 0,58). There is also rather high correlation with crime rate and the percentage of lower status of the population.
```{r}
# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d",tl.cex = 0.6)

```
  
## Setting up the data for analysis
From the data summary we made before we can see that the scales of the variables are very different. In order to analyze the data with linear discriminant analysis the data needs to be standardized. To do that  we subtract the column means from the corresponding columns and divide the difference with standard deviation. 
$$scaled(x) = \frac{x - mean(x)}{ sd(x)}$$
After centering and standardisising the data we can see that all the variables have mean = 0 and minimum and maximum values are of the same scale.

```{r}

# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)
```
When we scale the Boston data, the resulting object boston_scaled is "matrix" "array" instead of data frame*. In order to continue the object needs to be changed to data frame.

```{r}
# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)
```
Let's look at the crime rate variable "crim" in our scaled dataframe boston_scaled and create a new  categorial variable "crime" of the crime rate using the quantiles as break points. Now we can see that the new factor crime has four categories (low, med_low, med_high and high) and all of them have approximately same amount of observations.  
Let's then remove the old crime rate variable from the dataset and divide the data set to train and test sets. Now 80 % of the dataset belongs to the train set.

```{r}

# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim 
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]


```
## Linear discriminant analysis
Linear discriminant analysis is a method that is used to find a linear combination of features that characterizes or separates two or more classes of objects or events. LDA is related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas LDA has continuous independent variables and a categorical dependent variable (i.e. the class label).  
Here we fit the LDA to the train set using the categorial variable "crime" we just created as the target variable and all the other variables in the dataset as predictor variables.

 
```{r}

# linear discriminant analysis
lda.fit <- lda(crime ~., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```
  
  As we divided the boston_scaled dataset to train and test sets we can now test if the LDA can prdict the crime categories in the test set correctly. Let's start by saving the crime categories from the test set and then removing the categorical crime variable from the test dataset. Then we can predict the classes with the LDA model on the test data and cross tabulate the results with the correct crime categories from the test set.

```{r}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```
The LDA model manages to predict the higher crime rates rather well. The categor "high" is predicted 100% right and in the "med_high" category 18/26 = 69% of observations is predicted correctly and the 31% is predicted to be "med_low". The "med_low" category has 22 observations of which 15 (68%) is predicted correctly. Four observations (18%) are predicted as "low" and 3 (14%) as "med_high". The prediction of the "low" category is not as good. It has 27 observations and only 15 of them (56%) is predicted correctly. 37% of the observations in category "low" are predicted as "med_low" and two observations (7%) even as "med_high". To summarize, the model manages to predict the higher crime rates well but is rather bad at predicting low crime rate.

## K-means clustering
The k-means clustering is a method, that assigns observations to groups or *clusters* based on similarity of the objects. Similarity or dissimilarity of objects can be measured with distance measures. Let's load the Boston dataset again, standardize it and then calculate euclidean distance matrix and manhattan distance matrix.  

```{r}
library(MASS)
data("Boston")

# center and standardize variables
boston_scaled2 <- scale(Boston)

# euclidean distance matrix
dist_eu <- dist(boston_scaled2)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled2, method = "manhattan")

# look at the summary of the distances
summary(dist_man)

```
Let's try clustering the boston_scaled2 data with three clusters. The most commonly used implementation of k-means clustering is one that tries to find the partition of the *n* individuals into *k* groups that minimizes the within-group sum of squares (WGSS) over all variables. In the plot the colors are based on the clusters that k-means function produced.
   
 
```{r}

# k-means clustering
km <- kmeans(boston_scaled2, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_scaled2, col = km$cluster)

```
  
When trying to interpret the plots I really can't see three distinct clusters in most of the graphs. The number of clusters was just guessed here, so maybe three clusters is not the right number. The “true” value of *k* is usually evalueted by considering every possible partition of the *n* individuals into *k* groups, and then selecting the one with the lowest within-group sum of squares (WGSS). Here we investigate what is the optimal number of clusters by plotting the total within-groups sum of squares for one- to ten-group solutions. The ideal number of clusters is the number where the plot suddenly drops and forms so called "elbow".

```{r}
library(ggplot2)
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twgss <- sapply(1:k_max, function(k){kmeans(boston_scaled2, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twgss, geom = 'line')



```
  
According to the plot it seems that the "true" number of clusters is two. Let's run the k-means algorithm again with two clusters and visualize the variables again.
```{r}

# k-means clustering
km <- kmeans(boston_scaled2, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled2, col = km$cluster)

```
  
The two-group solution seems better. At least in some graphs there seems to be two rather distinct clusters. For example here is the nitrogen oxides concentration vs. accessibility to radial highways.
```{r}
plot(nox~rad, data=boston_scaled2, main="Nitrogen oxides concentration vs. accessibility to radial highways", xlab="rad", ylab="nox", col = km$cluster)

```
